{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self.make_layer(64, 64, 2)\n",
    "        self.layer2 = self.make_layer(64, 64, 2)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "\n",
    "    def make_layer(self, in_channels, out_channels, blocks):\n",
    "        layers = []\n",
    "        for _ in range(blocks):\n",
    "            layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleResNet().to(device)\n",
    "model.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, train_loader, num_epochs):\n",
    "    total_step = len(train_loader)\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (i+1) % 100 == 0:\n",
    "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                       .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        accuracy = correct / total * 100\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model, criterion, optimizer, train_loader, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_accuracy = test_model(model, test_loader)\n",
    "print(f\"Baseline Accuracy: {baseline_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pruning rates for evaluation\n",
    "pruning_rates = [0.2, 0.5, 0.8]\n",
    "\n",
    "for prune_rate in pruning_rates:\n",
    "        # Prune nodes using PCA\n",
    "    def prune_nodes_PCA(model, prune_ratio):\n",
    "        # Extracting weights\n",
    "        weights = torch.cat([param.view(-1) for param in model.parameters()]).cpu().detach().numpy()\n",
    "\n",
    "        # Print weights for debugging\n",
    "        print(\"Weights before PCA pruning:\", weights)\n",
    "\n",
    "        # Applying PCA\n",
    "        pca = PCA(n_components=1)\n",
    "        pca.fit(weights.reshape(-1, 1))\n",
    "        explained_variance_ratio = np.sum(pca.explained_variance_ratio_)\n",
    "\n",
    "        # Print explained variance for debugging\n",
    "        print(\"Explained Variance Ratio (PCA):\", explained_variance_ratio)\n",
    "\n",
    "        node_threshold = np.percentile(np.abs(weights), prune_ratio * 100)\n",
    "\n",
    "        # Print node threshold for debugging\n",
    "        print(\"Node Threshold (PCA):\", node_threshold)\n",
    "\n",
    "        # Prune nodes\n",
    "        for param in model.parameters():\n",
    "            param.data[torch.abs(param.data) < node_threshold] = 0\n",
    "\n",
    "        return explained_variance_ratio\n",
    "\n",
    "    explained_variance_ratio_nodes_pca = prune_nodes_PCA(model, prune_rate)\n",
    "    accuracy_nodes_pruned_pca = test_model(model, test_loader)\n",
    "    print(f\"PCA Pruning Rate: {prune_rate}, Accuracy after pruning nodes: {accuracy_nodes_pruned_pca:.2f}%\")\n",
    "    print(f\"Explained Variance Ratio (Nodes - PCA): {explained_variance_ratio_nodes_pca:.2f}\")\n",
    "\n",
    "    # Prune connections using PCA\n",
    "    def prune_connections_PCA(model, prune_ratio):\n",
    "        # Extracting weights\n",
    "        weights = torch.cat([param.view(-1) for param in model.parameters()]).cpu().detach().numpy()\n",
    "\n",
    "        # Print weights for debugging\n",
    "        print(\"Weights before PCA pruning:\", weights)\n",
    "\n",
    "        # Applying PCA\n",
    "        pca = PCA(n_components=1)\n",
    "        pca.fit(weights.reshape(-1, 1))\n",
    "        explained_variance_ratio = np.sum(pca.explained_variance_ratio_)\n",
    "\n",
    "        # Print explained variance for debugging\n",
    "        print(\"Explained Variance Ratio (PCA):\", explained_variance_ratio)\n",
    "\n",
    "        weight_threshold = np.percentile(np.abs(weights), prune_ratio * 100)\n",
    "\n",
    "        # Print node threshold for debugging\n",
    "        print(\"Weights Threshold (PCA):\", weight_threshold)\n",
    "\n",
    "        # Prune connections\n",
    "        for param in model.parameters():\n",
    "            param.data[torch.abs(param.data) < weight_threshold] = 0\n",
    "\n",
    "        return explained_variance_ratio\n",
    "\n",
    "    explained_variance_ratio_connections_pca = prune_connections_PCA(model, prune_rate)\n",
    "    accuracy_connections_pruned_pca = test_model(model, test_loader)\n",
    "    print(f\"PCA Pruning Rate: {prune_rate}, Accuracy after pruning connections: {accuracy_connections_pruned_pca:.2f}%\")\n",
    "    print(f\"Explained Variance Ratio (Connections - PCA): {explained_variance_ratio_connections_pca:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
